diff --git a/__pycache__/A3D_MIL_dataset.cpython-36.pyc b/__pycache__/A3D_MIL_dataset.cpython-36.pyc
index 6cbd649..406d6b0 100644
Binary files a/__pycache__/A3D_MIL_dataset.cpython-36.pyc and b/__pycache__/A3D_MIL_dataset.cpython-36.pyc differ
diff --git a/__pycache__/pred_head.cpython-36.pyc b/__pycache__/pred_head.cpython-36.pyc
index 8e146dc..02c676d 100644
Binary files a/__pycache__/pred_head.cpython-36.pyc and b/__pycache__/pred_head.cpython-36.pyc differ
diff --git a/__pycache__/trainer.cpython-36.pyc b/__pycache__/trainer.cpython-36.pyc
index 3c68be8..7581e43 100644
Binary files a/__pycache__/trainer.cpython-36.pyc and b/__pycache__/trainer.cpython-36.pyc differ
diff --git a/train_MIL.py b/train_MIL.py
index 40ae250..a7197b7 100644
--- a/train_MIL.py
+++ b/train_MIL.py
@@ -5,6 +5,9 @@ from pred_head import PredHead
 from A3D_MIL_dataset import A3DMILDataset
 from tqdm import tqdm
 import torch.nn.functional as F
+import wandb
+
+wandb.init() 
 
 use_cuda = torch.cuda.is_available()
 
@@ -24,6 +27,8 @@ test_loader = data.DataLoader(training_set, **params)
 
 net = PredHead() 
 net.cuda() 
+wandb.watch(net) 
+
 for params in net.parameters():
     params.requires_grad = True
 
@@ -35,11 +40,11 @@ def loss_fn(outputs, labels):
 
 optimizer = torch.optim.SGD(net.parameters(), lr = 0.001)
 # optimizer = torch.optim.Adam(net.parameters(), lr = 0.0001 )
-
+net.train() 
 for epoch in range(max_epochs):
     # Training
     running_loss = 0.0
-    net.train() 
+    # net.train() 
     for idx, (local_batch, local_labels) in enumerate(tqdm(data_loader)):
         # Transfer to GPU
         local_batch, local_labels = local_batch.to(device), local_labels.to(device)
@@ -51,7 +56,9 @@ for epoch in range(max_epochs):
         running_loss += (loss.item() - running_loss) / (idx+1)
         loss.backward()
         optimizer.step()
-    print(running_loss) 
+        if idx % 10 == 0:
+            wandb.log({"loss": running_loss}) 
+    # print(running_loss) 
     correct = 0
     total = 0
     print("=========begin to eval==========") 
